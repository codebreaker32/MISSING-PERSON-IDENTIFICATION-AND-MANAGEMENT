{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codebreaker32/MISSING-PERSON-IDENTIFICATION-AND-MANAGEMENT/blob/main/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-qHOt2zvMvQ"
      },
      "source": [
        "# Image Captioning\n",
        "## Introduction\n",
        "This assignment aims to describe the content of an image by using CNNs and RNNs to build an Image Caption Generator. The model would be based on the paper [4] and it will be implemented using Tensorflow and Keras. The dataset used is Flickr 8K [5], consisting of 8,000 images each one paired with five different captions to provide clear descriptions.\n",
        "\n",
        "The model architectures consists of a CNN which extracts the features and encodes the input image and a Recurrent Neural Network (RNN) based on Long Short Term Memory (LSTM) layers. The most significant difference with other models is that the image embedding is provided as the first input to the RNN network and only once.\n",
        "![Model Architecture](https://github.com/raunak222/Image-Captioning/raw/master/Image/decoder.png)\n",
        "\n",
        "\n",
        "## References\n",
        "[1] Image captioning with visual attention: https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "\n",
        "[2] RNNs in Computer Vision â€” Image captioning: https://thinkautonomous.medium.com/rnns-in-computer-vision-image-captioning-597d5e1321d1\n",
        "\n",
        "[3] Image Captioning Project from Udacity Computer Vision Nanodegree: https://github.com/raunak222/Image-Captioning\n",
        "\n",
        "[4] Show and Tell: A Neural Image Caption Generator: https://arxiv.org/pdf/1411.4555.pdf\n",
        "\n",
        "[5] Flickr 8k Dataset: https://www.kaggle.com/adityajn105/flickr8k\n",
        "\n",
        "## Authors\n",
        "- Serghei Socolovschi: [serghei@kth.se](mailto:serghei@kth.se)\n",
        "- Angel Igareta: [alih2@kth.se](mailto:alih2@kth.se)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt3hHGrEwcFm"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bMguKrcSRBGy",
        "outputId": "cce1a938-169e-4208-8a28-30647e991893"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfjrnYN0vLpa"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "from time import time\n",
        "\n",
        "from tqdm import tqdm # progress bar\n",
        "from sklearn.model_selection import train_test_split # Dividing train test\n",
        "from nltk.translate.bleu_score import corpus_bleu # BLEU Score"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w6rfmajwbEk"
      },
      "source": [
        "## Dataset\n",
        "Load dataset from local path or google drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()   # Upload kaggle.json here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "RKaiyiZtMle2",
        "outputId": "0d46a1e7-b66d-4bd0-d251-138738d2f011"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bc66ca78-4dcd-4e6d-aa72-8aad4a410b62\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bc66ca78-4dcd-4e6d-aa72-8aad4a410b62\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2397724048.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Upload kaggle.json here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json   # secure permissions\n"
      ],
      "metadata": {
        "id": "c4wV68uRMvhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d adityajn105/flickr8k"
      ],
      "metadata": {
        "id": "oeucG2uAMyQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip flickr8k.zip"
      ],
      "metadata": {
        "id": "516Up6uwNUtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ANmV2vRLJhfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwKGCUZlxwEz"
      },
      "source": [
        "dataset_path = \"/content\"\n",
        "dataset_images_path = os.path.join(dataset_path, \"Images/\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93wtzSn0MbAz"
      },
      "source": [
        "Images configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOxSmplF3Jdc"
      },
      "source": [
        "img_height = 180\n",
        "img_width = 180\n",
        "validation_split = 0.2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9js_Vnr3VTN"
      },
      "source": [
        "### Encoder Model\n",
        "\n",
        "In order to extract the features from the images, a pretrained CNN model, named Inception V3 was used. In the figure below, there is the representation of the architecture of the used network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mRYvH-lKjge"
      },
      "source": [
        "# Remove the last layer of the Inception V3 model\n",
        "def get_encoder():\n",
        "    image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "    new_input = image_model.input\n",
        "    hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        "    return image_features_extract_model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUp3k4tv3QUK"
      },
      "source": [
        "### Read captions\n",
        "Create dictionary with picture filename as the key and an array of captions as the value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmbJa3Q-Kjgb"
      },
      "source": [
        "# Preprocess the caption, splitting the string and adding <start> and <end> tokens\n",
        "def get_preprocessed_caption(caption):\n",
        "    caption = re.sub(r'\\s+', ' ', caption)\n",
        "    caption = caption.strip()\n",
        "    caption = \"<start> \" + caption + \" <end>\"\n",
        "    return caption"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "tU0dd06gOHS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inAwp6UWw42K"
      },
      "source": [
        "images_captions_dict = {}\n",
        "\n",
        "with open(os.path.join(dataset_path, \"captions.txt\"), \"r\") as dataset_info:\n",
        "    next(dataset_info) # Omit header: image, caption\n",
        "\n",
        "    # Using a subset of 4,000 entries out of 40,000\n",
        "    for info_raw in list(dataset_info)[:4000]:\n",
        "        info = info_raw.split(\",\")\n",
        "        image_filename = info[0]\n",
        "        caption = get_preprocessed_caption(info[1])\n",
        "\n",
        "        if image_filename not in images_captions_dict.keys():\n",
        "            images_captions_dict[image_filename] = [caption]\n",
        "        else:\n",
        "            images_captions_dict[image_filename].append(caption)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF_Jv1AKKjgf"
      },
      "source": [
        "### Read images\n",
        "Create dictionary with image filename as key and the image feature extracted using the pretrained model as the value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLtoBOSwNnr7"
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(dataset_images_path + image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (img_height, img_width))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img) # preprocessing needed for pre-trained model\n",
        "    return img, image_path"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtV_P5lY7K-x"
      },
      "source": [
        "image_captions_dict_keys = list(images_captions_dict.keys())\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(image_captions_dict_keys)\n",
        "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO95FUrAKjgs",
        "outputId": "b428b6f2-7cb6-4a46-8129-892cbcbf849e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "images_dict = {}\n",
        "encoder = get_encoder()\n",
        "for img_tensor, path_tensor in tqdm(image_dataset):\n",
        "    batch_features_tensor = encoder(img_tensor)\n",
        "\n",
        "    # Loop over batch to save each element in images_dict\n",
        "    for batch_features, path in zip(batch_features_tensor, path_tensor):\n",
        "        decoded_path = path.numpy().decode(\"utf-8\")\n",
        "        images_dict[decoded_path] = batch_features.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:42<00:17,  4.42s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPRFVbOiM4r8"
      },
      "source": [
        "Image size after extracting features from the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUjh7BL0Kjgu"
      },
      "source": [
        "list(images_dict.items())[0][1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_NC5T0UKjg9"
      },
      "source": [
        "Display image from original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weVyhtW7KjhC"
      },
      "source": [
        "plt.imshow(load_image('1000268201_693b08cb0e.jpg')[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnPrQKqI-dqr"
      },
      "source": [
        "### Get images and labels from filenames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g-WXG-e-i5A",
        "scrolled": true
      },
      "source": [
        "def get_images_labels(image_filenames):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for image_filename in image_filenames:\n",
        "        image = images_dict[image_filename]\n",
        "        captions = images_captions_dict[image_filename]\n",
        "\n",
        "        # Add one instance per caption\n",
        "        for caption in captions:\n",
        "            images.append(image)\n",
        "            labels.append(caption)\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh79Z4VGAsjt"
      },
      "source": [
        "### Generate train and test set\n",
        "This approach divides image_filenames, to avoid same image with different caption in train and test dataset. Also the resulting train test is not shuffled because a tensorflow native method will be used for that aim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmv4Ov2zAukn"
      },
      "source": [
        "image_filenames = list(images_captions_dict.keys())\n",
        "image_filenames_train, image_filenames_test = \\\n",
        "    train_test_split(image_filenames, test_size=validation_split, random_state=1)\n",
        "\n",
        "X_train, y_train_raw = get_images_labels(image_filenames_train)\n",
        "X_test, y_test_raw = get_images_labels(image_filenames_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v9iXLjjKjg2"
      },
      "source": [
        "# Per image 5 captions and 0.2 test split\n",
        "len(X_train), len(y_train_raw), len(X_test), len(y_test_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zchQR7L8Kjg3"
      },
      "source": [
        "### Tokenize train labels\n",
        "Generate a vocabulary and transform the train captions to a vector with their indices in the vocabulary [1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVlQXxzqKjg4"
      },
      "source": [
        "top_k = 5000 # Take maximum of words out of 7600\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "\n",
        "# Generate vocabulary from train captions\n",
        "tokenizer.fit_on_texts(y_train_raw)\n",
        "\n",
        "# Introduce padding to make the captions of the same size for the LSTM model\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "# Create the tokenized vectors\n",
        "y_train = tokenizer.texts_to_sequences(y_train_raw)\n",
        "\n",
        "# Add padding to each vector to the max_length of the captions (automatically done)\n",
        "y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVCzNLB8PeYk"
      },
      "source": [
        "Calculate max caption length which would be the number of hidden layers in the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tei4umndKjg5"
      },
      "source": [
        "max_caption_length = max(len(t) for t in y_train)\n",
        "print(max_caption_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqr8VoNiPone"
      },
      "source": [
        "Example tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoUCBtDwKjg6"
      },
      "source": [
        "[tokenizer.index_word[i] for i in y_train[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69SGNQeIKjg7"
      },
      "source": [
        "### Generate Tensorflow dataset\n",
        "Generate dataset using buffer and batch size that would be used during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50YbII04Kjg7"
      },
      "source": [
        "  dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2MoZKmwKjg8"
      },
      "source": [
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 64\n",
        "NUM_STEPS = BUFFER_SIZE // BATCH_SIZE\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# Using prefetching: https://www.tensorflow.org/guide/data_performance#prefetching\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5B520033IUU"
      },
      "source": [
        "## Models Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CsiHTr6QKfb"
      },
      "source": [
        "### CNN Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6VzCSzgKjhF"
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.flat = tf.keras.layers.Flatten()\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim) #, activation='relu')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.flat(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No8kXAyFQOjR"
      },
      "source": [
        "### RNN Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk0SatimKjhG"
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        # input_dim = size of the vocabulary\n",
        "        # Define the embedding layer to transform the input caption sequence\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Define the Long Short Term Memory layer to predict the next words in the sequence\n",
        "        self.lstm = tf.keras.layers.LSTM(self.units, return_sequences=True, return_state=True)\n",
        "\n",
        "        # Define a dense layer to transform the LSTM output into prediction of the best word\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size) #, activation='softmax')\n",
        "\n",
        "    # A function that transforms the input embeddings and passes them to the LSTM layer\n",
        "    def call(self, captions, features, omit_features = False, initial_state = None, verbose = False):\n",
        "        if verbose:\n",
        "            print(\"Before embedding\")\n",
        "            print(captions.shape)\n",
        "\n",
        "        embed = self.embedding(captions) #(batch_size, 1, embedding_dim)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Embed\")\n",
        "            print(embed.shape)\n",
        "\n",
        "        features = tf.expand_dims(features, 1)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Features\")\n",
        "            print(features.shape)\n",
        "\n",
        "        # Concatenating the image and caption embeddings before providing them to LSTM\n",
        "        # shape == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        lstm_input = tf.concat([features, embed], axis=-2) if (omit_features == False) else embed\n",
        "\n",
        "        if verbose:\n",
        "            print(\"LSTM input\")\n",
        "            print(lstm_input.shape)\n",
        "\n",
        "        # Passing the concatenated vector to the LSTM\n",
        "        output, memory_state, carry_state = self.lstm(lstm_input, initial_state=initial_state)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"LSTM output\")\n",
        "            print(output.shape)\n",
        "\n",
        "        # Transform LSTM output units to vocab_size\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, memory_state, carry_state\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_E2G-rzKjhJ"
      },
      "source": [
        "## Train Stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjbE226nKjhJ"
      },
      "source": [
        "units = embedding_dim = 512 # As in the paper\n",
        "vocab_size = min(top_k + 1, len(tokenizer.word_index.keys()))\n",
        "\n",
        "# Initialize encoder and decoder\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# As the label is not one-hot encoded but indices. Logits as they are not probabilities.\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# Computes the loss using SCCE and calculates the average of singular losses in the tensor\n",
        "def loss_function(real, pred, verbose=False):\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Loss\")\n",
        "        print(loss_)\n",
        "\n",
        "    loss_ = tf.reduce_mean(loss_, axis = 1)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"After Mean Axis 1\")\n",
        "        print(loss_)\n",
        "\n",
        "    return loss_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V257MWTlKjhL"
      },
      "source": [
        "# Key Point: Any Python side-effects (appending to a list, printing with print, etc) will only happen once, when func is traced.\n",
        "# To have side-effects executed into your tf.function they need to be written as TF ops:\n",
        "@tf.function\n",
        "def train_step(img_tensor, target, verbose=False):\n",
        "    if verbose:\n",
        "        print(\"Image tensor\")\n",
        "        print(img_tensor.shape)\n",
        "\n",
        "        print(\"Target\")\n",
        "        print(target.shape)\n",
        "\n",
        "    # The input would be each set of words without the last one (<end>), to leave space for the first one that\n",
        "    # would be the image embedding\n",
        "    dec_input = tf.convert_to_tensor(target[:, :-1])\n",
        "\n",
        "    # Source: https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Features CNN\")\n",
        "            print(features)\n",
        "\n",
        "        predictions, _, _ = decoder(dec_input, features, verbose=verbose)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Predictions RNN\")\n",
        "            print(predictions)\n",
        "\n",
        "        caption_loss = loss_function(target, predictions) # (batch_size, )\n",
        "\n",
        "        # After tape\n",
        "        total_batch_loss = tf.reduce_sum(caption_loss) # Sum (batch_size, ) => K\n",
        "        mean_batch_loss = tf.reduce_mean(caption_loss) # Mean(batch_size, ) => K\n",
        "\n",
        "    # Updated the variables\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(caption_loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return total_batch_loss, mean_batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dosj4FzKjhO"
      },
      "source": [
        "### Checkpoint\n",
        "Create a tensorflow checkpoint on a local path to save the encoder and decoder state while training. Only the last 5 models would be saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrkxMRSzKjhO"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-y18RcyKjhR"
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "    # restoring the latest checkpoint in checkpoint_path\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bZ08OVVKjhS"
      },
      "source": [
        "### Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Mx3RAvKjhX"
      },
      "source": [
        "loss_plot = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "nwJQ1x1UKjhh"
      },
      "source": [
        "EPOCHS = 5\n",
        "start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    real_epoch = len(loss_plot) + 1\n",
        "    start = time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        total_batch_loss, mean_batch_loss = train_step(img_tensor, target, verbose=False)\n",
        "        total_loss += total_batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Batch Loss {:.4f}'.format(real_epoch, batch, mean_batch_loss.numpy()))\n",
        "\n",
        "    print ('Total Loss {:.6f}'.format(total_loss))\n",
        "    epoch_loss = total_loss / NUM_STEPS\n",
        "\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(epoch_loss)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Epoch Loss {:.6f}'.format(real_epoch, epoch_loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "muLs-p4dKjhi"
      },
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFy0s0g-Kjhj"
      },
      "source": [
        "## Test Stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcyYIq3oQhh5"
      },
      "source": [
        "### Evaluate random image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s_q1Tn-Kjhj"
      },
      "source": [
        "# Remove <start>, <end> and <pad> marks from the predicted sequence\n",
        "def clean_caption(caption):\n",
        "    return [item for item in caption if item not in ['<start>', '<end>', '<pad>']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d4ocl2tKjhk"
      },
      "source": [
        "test_img_name = random.choice(image_filenames_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "J1RGDtzEKjhk"
      },
      "source": [
        "# Get captions from a test image\n",
        "def get_caption(img):\n",
        "    # Add image to an array to simulate batch size of 1\n",
        "    features = encoder(tf.expand_dims(img, 0))\n",
        "\n",
        "    caption = []\n",
        "    dec_input = tf.expand_dims([], 0)\n",
        "\n",
        "     # Inputs the image embedding into the trained LSTM layer and predicts the first word of the sequence.\n",
        "    # The output, hidden and cell states are passed again to the LSTM to generate the next word.\n",
        "    # The iteration is repeated until the caption does not reach the max length.\n",
        "    state = None\n",
        "    for i in range(1, max_caption_length):\n",
        "        predictions, memory_state, carry_state = \\\n",
        "            decoder(dec_input, features, omit_features=i > 1, initial_state=state)\n",
        "\n",
        "        # Takes maximum index of predictions\n",
        "        word_index = np.argmax(predictions.numpy().flatten())\n",
        "\n",
        "        caption.append(tokenizer.index_word[word_index])\n",
        "\n",
        "        dec_input = tf.expand_dims([word_index], 0)\n",
        "        state = [memory_state, carry_state]\n",
        "\n",
        "    # Filter caption\n",
        "    return clean_caption(caption)\n",
        "\n",
        "raw_img = load_image(test_img_name)[0]\n",
        "img = images_dict[test_img_name]\n",
        "captions = images_captions_dict[test_img_name]\n",
        "\n",
        "plt.imshow(raw_img)\n",
        "\n",
        "print(\"Real captions\")\n",
        "for caption in captions:\n",
        "    print(caption)\n",
        "\n",
        "print(\"Esimated caption\")\n",
        "estimated_caption = get_caption(img)\n",
        "print(estimated_caption)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_q10vOfKjhl"
      },
      "source": [
        "### Evaluate dataset using BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Vdz4rw38Kjhl"
      },
      "source": [
        "def get_caption(img):\n",
        "    # Add image to an array to simulate batch size of 1\n",
        "    features = encoder(tf.expand_dims(img, 0))\n",
        "\n",
        "    caption = []\n",
        "    dec_input = tf.expand_dims([], 0)\n",
        "\n",
        "    state = None\n",
        "    for i in range(1, max_caption_length):\n",
        "        predictions, memory_state, carry_state = \\\n",
        "            decoder(dec_input, features, omit_features=i > 1, initial_state=state)\n",
        "\n",
        "        word_index = np.argmax(predictions.numpy().flatten())\n",
        "\n",
        "        caption.append(tokenizer.index_word[word_index])\n",
        "\n",
        "        dec_input = tf.expand_dims([word_index], 0)\n",
        "        state = [memory_state, carry_state]\n",
        "\n",
        "    # Filter caption\n",
        "    return clean_caption(caption)\n",
        "\n",
        "actual, predicted = [], []\n",
        "\n",
        "for test_img_name in image_filenames_test:\n",
        "    img = images_dict[test_img_name]\n",
        "    estimated_caption = get_caption(img)\n",
        "\n",
        "    captions = [clean_caption(caption.split()) for caption in images_captions_dict[test_img_name]]\n",
        "\n",
        "    # store actual and predicted\n",
        "    actual.append(captions)\n",
        "    predicted.append(estimated_caption)\n",
        "\n",
        "# Print BLEU score\n",
        "print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}